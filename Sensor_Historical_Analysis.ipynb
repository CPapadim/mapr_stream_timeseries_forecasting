{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Batch IoT Sensor Forecasting using TensorFlow and RNN\n",
    "Author:  Justin Brandenburg, Data Scientist\n",
    "Email:   jbrandenburg@mapr.com\n",
    "\n",
    "Requirements: Python 3.5 and TensorFlow 1.X\n",
    "If these are not installed, this demo will not run\n",
    "\n",
    "To install TF:\n",
    "[user01@maprdemo ]$ conda install keras\n",
    "[user01@maprdemo ]$ conda install tensorflow\n",
    "\n",
    "This assumes that the XML files have been transformed into a csv file using Sensor_ETLsparksubmit.py\n",
    "\n",
    "This script is best demo'd in a jupyter notebook to show the data exploration, feature transformation, model training and\n",
    "testing on historical data that is in MapR-FS.  \n",
    "\n",
    "create a folder that will capture our saved TF model\n",
    "[user01@maprdemo ]$ mkdir rwTFmodel\n",
    "\n",
    "Start jupyter notebook and acces it via your browser at 127.0.0.1:9999\n",
    "[user01@maprdemo ]$ jupyter notebook\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following extra packages will be installed:\n",
      "  blt tk8.6-blt2.5\n",
      "Suggested packages:\n",
      "  blt-demo tix python-tk-dbg\n",
      "The following NEW packages will be installed:\n",
      "  blt python-tk tk8.6-blt2.5\n",
      "0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 627 kB of archives.\n",
      "After this operation, 2122 kB of additional disk space will be used.\n",
      "Get:1 http://httpredir.debian.org/debian/ jessie/main tk8.6-blt2.5 amd64 2.5.3+dfsg-1 [586 kB]\n",
      "Get:2 http://httpredir.debian.org/debian/ jessie/main blt amd64 2.5.3+dfsg-1 [14.3 kB]\n",
      "Get:3 http://httpredir.debian.org/debian/ jessie/main python-tk amd64 2.7.8-2+b1 [26.7 kB]\n",
      "Fetched 627 kB in 0s (866 kB/s)     \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Selecting previously unselected package tk8.6-blt2.5.\n",
      "(Reading database ... 75150 files and directories currently installed.)\n",
      "Preparing to unpack .../tk8.6-blt2.5_2.5.3+dfsg-1_amd64.deb ...\n",
      "Unpacking tk8.6-blt2.5 (2.5.3+dfsg-1) ...\n",
      "Selecting previously unselected package blt.\n",
      "Preparing to unpack .../blt_2.5.3+dfsg-1_amd64.deb ...\n",
      "Unpacking blt (2.5.3+dfsg-1) ...\n",
      "Selecting previously unselected package python-tk.\n",
      "Preparing to unpack .../python-tk_2.7.8-2+b1_amd64.deb ...\n",
      "Unpacking python-tk (2.7.8-2+b1) ...\n",
      "Setting up tk8.6-blt2.5 (2.5.3+dfsg-1) ...\n",
      "Setting up blt (2.5.3+dfsg-1) ...\n",
      "Setting up python-tk (2.7.8-2+b1) ...\n",
      "Processing triggers for libc-bin (2.19-18+deb8u10) ...\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!sudo apt-get install -y python-tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6b80581fb84c71be26626dcace06f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fb322ac5904cfa80c7706329d9d3c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "#import the data from MapR-FS, super easy\n",
    "#df = pd.read_csv(\"/user/user01/rw_etl.csv/part-00000-45866095-f76d-4f6c-ba2d-a07f0ab2dc04.csv\").sort_values(['TimeStamp'], ascending=True).reset_index()\n",
    "\n",
    "#TEMPORARY - load data from local container for testing purposes while cluster is down\n",
    "df = pd.read_csv(\"tmp/data/part-00000-45866095-f76d-4f6c-ba2d-a07f0ab2dc04.csv\").sort_values(['TimeStamp'], ascending=True).reset_index()\n",
    "\n",
    "df.drop(['::[scararobot]Ax_J1.PositionCommand','::[scararobot]Ax_J1.TorqueFeedback','::[scararobot]Ax_J2.PositionCommand','::[scararobot]Ax_J2.TorqueFeedback','::[scararobot]Ax_J3.TorqueFeedback','::[scararobot]Ax_J6.TorqueFeedback','::[scararobot]ScanTimeAverage','::[scararobot]Ax_J6.PositionCommand','::[scararobot]Ax_J3.PositionCommand','index'], axis=1, inplace=True)\n",
    "df['TimeStamp']=pd.to_datetime(df['TimeStamp'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#plot some of the columns to get an idea of the trends over time\n",
    "df.plot(x=\"TimeStamp\", y=\"::[scararobot]Ax_J1.ActualPosition\", kind=\"line\")\n",
    "df.plot(x=\"TimeStamp\", y=[\"::[scararobot]Ax_J1.ActualPosition\",\"::[scararobot]Ax_J3.TorqueCommand\"], kind=\"line\")\n",
    "df.plot(x=\"TimeStamp\", y=[\"::[scararobot]CS_Cartesian.ActualPosition[0]\",\"::[scararobot]CS_Cartesian.ActualPosition[1]\"], kind=\"line\")\n",
    "\n",
    "#remove rows that are all zeros\n",
    "df1 = df[df[\"::[scararobot]speed\"] != 0].set_index('TimeStamp')   \n",
    "print (len(df1))\n",
    "\n",
    "#create a new column that will be our feature variable for our model\n",
    "#df1['total']=df1.sum(axis=1)\n",
    "df1['Total']= df1.select_dtypes(include=['float64','float32']).apply(lambda row: np.sum(row),axis=1)\n",
    "\n",
    "#convert into a time series object\n",
    "ts = pd.Series(df1['Total'])\n",
    "ts.plot(c='b', title='RW Total Sensor Aggregation')\n",
    "\n",
    "#prepare data and inputs for our TF model\n",
    "num_periods = 100\n",
    "f_horizon = 1       #number of periods into the future we are forecasting\n",
    "TS = np.array(ts)   #convert time series object to an array\n",
    "print (TS[0:10])\n",
    "print (len(TS))\n",
    "\n",
    "#create our training input data set \"X\"\n",
    "x_data = TS[:(len(TS)-(len(TS) % num_periods))]\n",
    "print (x_data[0:5])\n",
    "x_batches = x_data.reshape(-1, num_periods, 1)\n",
    "print (len(x_batches))\n",
    "print (x_batches.shape)\n",
    "#print (x_batches[0:3])\n",
    "\n",
    "#create our training output dataset \"y\"\n",
    "y_data = TS[1:(len(TS)-(len(TS) % num_periods))+f_horizon]\n",
    "#print (y_data)\n",
    "#print (len(y_data))\n",
    "#y_data = TS[(num_periods+(f_horizon-1))::(num_periods)]\n",
    "print (y_data)\n",
    "print (len(y_data))\n",
    "y_batches = y_data.reshape(-1, num_periods, 1)\n",
    "print (len(y_batches))\n",
    "\n",
    "#create our test X and y data\n",
    "def test_data(series,forecast,num_periods):\n",
    "    test_x_setup = series[-(num_periods + forecast):]\n",
    "    testX = test_x_setup[:num_periods].reshape(-1, num_periods, 1)\n",
    "    testY = TS[-(num_periods):].reshape(-1, num_periods, 1)\n",
    "    return testX,testY\n",
    "\n",
    "X_test, Y_test = test_data(TS,f_horizon,num_periods)\n",
    "print (X_test.shape)\n",
    "print (X_test[:,(num_periods-1):num_periods])\n",
    "print (Y_test.shape)\n",
    "print (Y_test[:,(num_periods-1):num_periods])\n",
    "\n",
    "#import tensorflow libraries\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "#set up our TF model parameters\n",
    "\n",
    "tf.reset_default_graph()   #We didn't have any previous graph objects running, but this would reset the graphs\n",
    "\n",
    "inputs = 1            #number of vectors submitted\n",
    "hidden = 100          #number of neurons we will recursively work through, can be changed to improve accuracy\n",
    "output = 1            #number of output vectors\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_periods, inputs], name = \"X\")   #create variable objects\n",
    "y = tf.placeholder(tf.float32, [None, num_periods, output], name = \"y\")\n",
    "\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden, activation=tf.nn.relu)   #create our RNN object\n",
    "rnn_output, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)               #choose dynamic over static\n",
    "\n",
    "learning_rate = 0.001   #small learning rate so we don't overshoot the minimum\n",
    "#tf.app.flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "\n",
    "stacked_rnn_output = tf.reshape(rnn_output, [-1, hidden])           #change the form into a tensor\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_output, output)        #specify the type of layer (dense)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, num_periods, output])          #shape of results\n",
    " \n",
    "loss = tf.reduce_sum(tf.square(outputs - y),name='loss')    #define the cost function which evaluates the quality of our model\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)          #gradient descent method\n",
    "training_op = optimizer.minimize(loss)          #train the result of the application of the cost_function                                 \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epochs = 1000     #number of iterations or training cycles, includes both the FeedFoward and Backpropogation\n",
    "saver = tf.train.Saver()   #we are going to save the model\n",
    "DIR=\"/user/user01/rwTFmodel\"  #path where the model will be saved\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op, feed_dict={X: x_batches, y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: x_batches, y: y_batches})\n",
    "            print(ep, \"\\tMSE:\", mse) \n",
    "            \n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_test})\n",
    "    print(y_pred[:,(num_periods-1):num_periods])\n",
    "    saver.save(sess, os.path.join(DIR,\"RWsensorTFmodel\"),global_step = epochs)\n",
    "\n",
    "\n",
    "\n",
    "#Plot our test y data and our y-predicted forecast\n",
    "plt.title(\"Forecast vs Actual\", fontsize=14)\n",
    "plt.plot(pd.Series(np.ravel(Y_test)), \"bo\", markersize=10, label=\"Actual\")\n",
    "#plt.plot(pd.Series(np.ravel(Y_test)), \"w*\", markersize=10)\n",
    "plt.plot(pd.Series(np.ravel(y_pred)), \"r.\", markersize=10, label=\"Forecast\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time Periods\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
